{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PubMed microbiology and cancer classifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter to remove empty lines and file structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "begin_re = re.compile(\"^====\")\n",
    "\n",
    "def is_text(line):\n",
    "    line = line.strip()\n",
    "    if not line or begin_re.match(line):\n",
    "        return False\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark import keyword_only\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "\n",
    "class PunctuationStripper(Transformer, HasInputCol, HasOutputCol):\n",
    "    \n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, outputCol=None):\n",
    "        super(PunctuationStripper, self).__init__()\n",
    "        kwargs = self.__init__._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, outputCol=None):\n",
    "        kwargs = self.setParams._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        punct_re = re.compile(r'[^\\w\\s]', re.UNICODE)\n",
    "        \n",
    "        def strip(s):\n",
    "            return punct_re.sub('', s)\n",
    "        \n",
    "        out_col = self.getOutputCol()\n",
    "        in_col = dataset[self.getInputCol()]\n",
    "        mapper = udf(strip, StringType())\n",
    "        \n",
    "        return dataset.withColumn(out_col, mapper(in_col))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load PubMed and non-PubMed articles in to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, DoubleType, StringType\n",
    "schema = StructType([\n",
    "    StructField(\"fullText\", StringType(), True), StructField(\"category\", DoubleType(), False)\n",
    "])\n",
    "\n",
    "def load_article(category_name, category_id):\n",
    "    text_file = spark.sparkContext.textFile(\"{}/*\".format(category_name))\n",
    "    return text_file.filter(is_text).map(lambda l: (l, float(category_id))).toDF(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loading and preprocessing should be turned into Spark tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "micro_articles = load_article(\"microbiol-small\", 0)\n",
    "cancer_articles = load_article(\"cancer-small\", 1)\n",
    "otherMed_articles = load_article(\"others-small\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bio = micro_articles.unionAll(cancer_articles)\n",
    "inputData = bio.unionAll(otherMed_articles)\n",
    "(train, test) = inputData.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a workaround for pyspark not finding numpy, [taken from the GitHub issue](https://github.com/jupyter/docker-stacks/issues/109)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "os.environ['PYTHONPATH'] = ':'.join(sys.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "delete TFIDF add ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from pyspark.ml.feature import HashingTF, IDF\n",
    "from pyspark.ml.feature import CountVectorizer, Tokenizer, StringIndexer, NGram, VectorAssembler\n",
    "\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import OneVsRest\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "\n",
    "punctuation_stripper = PunctuationStripper(inputCol=\"fullText\", outputCol=\"strippedText\")\n",
    "tokenizer = Tokenizer(inputCol=\"strippedText\", outputCol=\"words\")\n",
    "# CountVectorizer and HashingTF both can be used to get term frequency vectors\n",
    "# cv = CountVectorizer(inputCol=\"words\", outputCol=\"rawFeatures\")\n",
    "\n",
    "# hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\")\n",
    "# idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "\n",
    "threeGram = NGram(n=3, inputCol=\"words\", outputCol=\"threeGrams\")\n",
    "fourGram = NGram(n=4, inputCol=\"words\", outputCol=\"fourGrams\")\n",
    "cv_3g = CountVectorizer(inputCol=\"threeGrams\", outputCol=\"threeGramsCountVector\")\n",
    "cv_4g = CountVectorizer(inputCol=\"fourGrams\", outputCol=\"fourGramsCountVector\")\n",
    "fc = VectorAssembler(inputCols=[\"threeGramsCountVector\", \"fourGramsCountVector\"], outputCol=\"ngramsCountVector\")\n",
    "\n",
    "nb = NaiveBayes(featuresCol=\"ngramsCountVector\", labelCol=\"category\", modelType=\"multinomial\")\n",
    "# instantiate the One Vs Rest Classifier.\n",
    "ovr = OneVsRest(classifier=nb, labelCol=\"category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF -> IDF -> NaiveBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[punctuation_stripper,tokenizer, threeGram, fourGram, cv_3g, cv_4g, fc, nb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ovrModel = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "score the model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = ovrModel.transform(test)\n",
    "micro_articles_count = test.filter(test['category'] == 0.0).count()\n",
    "print(\"micro_articles number:\")\n",
    "print(micro_articles_count)\n",
    "cancer_articles_count = test.filter(test['category'] == 1.0).count()\n",
    "print(\"cancer_articles number:\")\n",
    "print(cancer_articles_count)\n",
    "otherMed_articles_count = test.filter(test['category'] == 2.0).count()\n",
    "print(\"otherMed_articles number:\")\n",
    "print(otherMed_articles_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "obtain evaluator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error : 0.22568093385214005\n"
     ]
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\", labelCol=\"category\")\n",
    "# compute the classification error on test data.\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error : \" + str(1 - accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n",
      "|prediction|category|\n",
      "+----------+--------+\n",
      "|       2.0|     0.0|\n",
      "|       0.0|     0.0|\n",
      "|       2.0|     0.0|\n",
      "|       0.0|     0.0|\n",
      "|       0.0|     0.0|\n",
      "|       0.0|     0.0|\n",
      "|       1.0|     0.0|\n",
      "|       0.0|     0.0|\n",
      "|       0.0|     0.0|\n",
      "|       0.0|     0.0|\n",
      "|       0.0|     0.0|\n",
      "|       2.0|     0.0|\n",
      "|       0.0|     0.0|\n",
      "|       0.0|     0.0|\n",
      "|       2.0|     0.0|\n",
      "|       2.0|     0.0|\n",
      "|       2.0|     0.0|\n",
      "|       2.0|     0.0|\n",
      "|       2.0|     0.0|\n",
      "|       0.0|     0.0|\n",
      "+----------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  confusion matrix\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "predictionsAndLabelsNB = predictions.select(\"prediction\", \"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes  accuracy 2:\n",
      "0.77431906614786\n",
      "[[  65.    9.   40.]\n",
      " [   2.   59.   59.]\n",
      " [   4.    2.  274.]]\n"
     ]
    }
   ],
   "source": [
    "metricsNB = MulticlassMetrics(predictionsAndLabelsNB.rdd)\n",
    "\n",
    "accuracyNB2 = metricsNB.accuracy\n",
    "print(\"Naive Bayes accuracy:\")\n",
    "print(accuracyNB2)\n",
    "\n",
    "confusionMatrixNB = metricsNB.confusionMatrix().toArray()\n",
    "print(\"Confusion Matrix: \")\n",
    "print(confusionMatrixNB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try classifying a few basic sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|            fullText|prediction|\n",
      "+--------------------+----------+\n",
      "|Bactibilia has se...|       2.0|\n",
      "|Assessing the bil...|       0.0|\n",
      "|Thirty bile sampl...|       2.0|\n",
      "|Julius Caesar was...|       2.0|\n",
      "|big data analysis...|       2.0|\n",
      "|do you know snow ...|       2.0|\n",
      "+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf = spark.createDataFrame([(\"Bactibilia has several consequences to human health\", ),\n",
    "                            (\"Assessing the bile microbiology of patients with biliopancreatic diseases in order to identify bacteria and their possible infectious complications\", ),\n",
    "                            (\"Thirty bile samples from patients at mean age â‰ˆ57.7 years, mostly female (n=18), were assessed. \", ),\n",
    "                            (\"Julius Caesar was a Roman general\", ),\n",
    "                            (\"big data analysis is great\", ),\n",
    "                            (\"do you know snow crash\", ),\n",
    "                           ], [\"fullText\"])\n",
    "tf = ovrModel.transform(tf)\n",
    "tf.select(tf['fullText'], tf['prediction']).show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:spark]",
   "language": "python",
   "name": "conda-env-spark-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
