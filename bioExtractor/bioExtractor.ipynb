{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PubMed bio-artical classifier in PySpark\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter to remove empty lines and file structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "begin_re = re.compile(\"<doc.*>$\")\n",
    "end_re = re.compile(\"^</doc>$\")\n",
    "\n",
    "def is_text(line):\n",
    "    line = line.strip()\n",
    "    if not line or begin_re.match(line) or end_re.match(line):\n",
    "        return False\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark import keyword_only\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "\n",
    "class PunctuationStripper(Transformer, HasInputCol, HasOutputCol):\n",
    "    \n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, outputCol=None):\n",
    "        super(PunctuationStripper, self).__init__()\n",
    "        kwargs = self.__init__._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, outputCol=None):\n",
    "        kwargs = self.setParams._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        punct_re = re.compile(r'[^\\w\\s]', re.UNICODE)\n",
    "        \n",
    "        def strip(s):\n",
    "            return punct_re.sub('', s)\n",
    "        \n",
    "        out_col = self.getOutputCol()\n",
    "        in_col = dataset[self.getInputCol()]\n",
    "        mapper = udf(strip, StringType())\n",
    "        \n",
    "        return dataset.withColumn(out_col, mapper(in_col))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load PubMed and non-PubMed articles in to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, DoubleType, StringType\n",
    "schema = StructType([\n",
    "    StructField(\"fullText\", StringType(), True), StructField(\"category\", DoubleType(), False)\n",
    "])\n",
    "\n",
    "def load_article(category_name, category_id):\n",
    "    text_file = spark.sparkContext.textFile(\"{}/*\".format(category_name))\n",
    "    return text_file.filter(is_text).map(lambda l: (l, float(category_id))).toDF(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loading and preprocessing should be turned into Spark tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bio_articles = load_article(\"bio\", 0)\n",
    "other_articles = load_article(\"other\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|            fullText|category|\n",
      "+--------------------+--------+\n",
      "|Association of ma...|     0.0|\n",
      "|Pregnancy, delive...|     0.0|\n",
      "|Spermicidal activ...|     0.0|\n",
      "|Laboratory tests ...|     0.0|\n",
      "|The diagnosis of ...|     0.0|\n",
      "|Preliminary resul...|     0.0|\n",
      "|Cesarean hysterec...|     0.0|\n",
      "|This paper review...|     0.0|\n",
      "|               Skin.|     0.0|\n",
      "|A reevaluation of...|     0.0|\n",
      "|Puerperal tubal s...|     0.0|\n",
      "|A study of 1830 p...|     0.0|\n",
      "|Cesarean section-...|     0.0|\n",
      "|Experience with c...|     0.0|\n",
      "|Adrenalectomy for...|     0.0|\n",
      "|Problems involved...|     0.0|\n",
      "|The early diagnos...|     0.0|\n",
      "|Coarctation of th...|     0.0|\n",
      "|Haemorrhagic gang...|     0.0|\n",
      "|The epidemiologic...|     0.0|\n",
      "+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bio_articles.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = bio_articles.unionAll(other_articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a workaround for pyspark not finding numpy, [taken from the GitHub issue](https://github.com/jupyter/docker-stacks/issues/109)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "os.environ['PYTHONPATH'] = ':'.join(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer, Tokenizer\n",
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "punctuation_stripper = PunctuationStripper(inputCol=\"fullText\", outputCol=\"strippedText\")\n",
    "tokenizer = Tokenizer(inputCol=\"strippedText\", outputCol=\"words\")\n",
    "# CountVectorizer and HashingTF both can be used to get term frequency vectors\n",
    "# cv = CountVectorizer(inputCol=\"words\", outputCol=\"rawFeatures\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF -> IDF -> NaiveBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\")\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "\n",
    "nb = NaiveBayes(featuresCol=\"features\", labelCol=\"category\", modelType=\"multinomial\")\n",
    "\n",
    "pipeline = Pipeline(stages=[punctuation_stripper, tokenizer, hashingTF, idf, nb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = pipeline.fit(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try classifying a few basic sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|            fullText|prediction|\n",
      "+--------------------+----------+\n",
      "|Bactibilia has se...|       0.0|\n",
      "|Assessing the bil...|       0.0|\n",
      "|Thirty bile sampl...|       0.0|\n",
      "|Julius Caesar was...|       1.0|\n",
      "|big data analysis...|       1.0|\n",
      "|do you know snow ...|       1.0|\n",
      "+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf = spark.createDataFrame([(\"Bactibilia has several consequences to human health\", ),\n",
    "                            (\"Assessing the bile microbiology of patients with biliopancreatic diseases in order to identify bacteria and their possible infectious complications\", ),\n",
    "                            (\"Thirty bile samples from patients at mean age â‰ˆ57.7 years, mostly female (n=18), were assessed. \", ),\n",
    "                            (\"Julius Caesar was a Roman general\", ),\n",
    "                            (\"big data analysis is great\", ),\n",
    "                            (\"do you know snow crash\", ),\n",
    "                           ], [\"fullText\"])\n",
    "tf = model.transform(tf)\n",
    "tf.select(tf['fullText'], tf['prediction']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:spark]",
   "language": "python",
   "name": "conda-env-spark-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
